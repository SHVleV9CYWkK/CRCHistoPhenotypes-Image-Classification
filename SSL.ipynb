{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T19:28:35.160894Z",
     "end_time": "2023-04-27T19:28:35.170046Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Image_classification_data/data_labels_mainData.csv')\n",
    "# data['isCancerous'] = data['isCancerous'].astype(str)\n",
    "# data['cellType'] = data['cellType'].astype(str)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T19:28:35.163999Z",
     "end_time": "2023-04-27T19:28:35.186283Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "class_samples_isCancerous = train_data['isCancerous'].value_counts()\n",
    "total_samples = np.sum(class_samples_isCancerous)\n",
    "class_weights_isCancerous = total_samples / class_samples_isCancerous\n",
    "class_weight_dict_isCancerous = {int(k): v for k, v in class_weights_isCancerous.to_dict().items()}\n",
    "\n",
    "class_samples_cellType = train_data['cellType'].value_counts()\n",
    "total_samples = np.sum(class_samples_cellType)\n",
    "class_weights_cellType = total_samples / class_samples_cellType\n",
    "class_weight_dict_cellType = {int(k): v for k, v in class_weights_cellType.to_dict().items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T19:28:35.175077Z",
     "end_time": "2023-04-27T19:28:35.186454Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1980 validated image filenames belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "test_data['cellType'] = test_data['cellType'].astype(str)\n",
    "validation_cellType_generator = test_datagen.flow_from_dataframe(\n",
    "    test_data,\n",
    "    directory='./Image_classification_data/patch_images',\n",
    "    x_col='ImageName',\n",
    "    y_col='cellType',\n",
    "    target_size=(27, 27),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T19:28:35.178361Z",
     "end_time": "2023-04-27T19:28:35.201931Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_100 (Conv2D)         (None, 27, 27, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_150 (Ba  (None, 27, 27, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_101 (Conv2D)         (None, 25, 25, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_151 (Ba  (None, 25, 25, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_50 (MaxPoolin  (None, 12, 12, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_102 (Conv2D)         (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_152 (Ba  (None, 12, 12, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_103 (Conv2D)         (None, 10, 10, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_153 (Ba  (None, 10, 10, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_51 (MaxPoolin  (None, 5, 5, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_25 (Flatten)        (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 512)               819712    \n",
      "                                                                 \n",
      " batch_normalization_154 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_155 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,154,852\n",
      "Trainable params: 1,152,420\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (27, 27, 3)\n",
    "num_classes = 4\n",
    "l2_coeff = 0.01\n",
    "\n",
    "model_categorical = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same',\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(l2_coeff)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_coeff)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(l2_coeff)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_coeff)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_coeff)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_coeff)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model_categorical.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T19:28:35.208629Z",
     "end_time": "2023-04-27T19:28:35.298851Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DI/HD 使用data_labels_extraData对多分类的模型进行增强\n",
    "通过半监督学习。我们将采用UDA。\n",
    "首先先获取额外的数据集并且进行相关处理。\n",
    "我们从数据集可以观察到，没有癌症在多分类中为2。所以我们可以将不是癌症的样本之间指定其多分类的类别为2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-27T19:28:35.306352Z",
     "end_time": "2023-04-27T19:28:35.885669Z"
    }
   },
   "outputs": [],
   "source": [
    "model_categorical = tf.keras.models.load_model('saved_model/model_categorical')\n",
    "data_extra = pd.read_csv('./Image_classification_data/data_labels_extraData.csv')\n",
    "data_extra['isCancerous'] = data_extra['isCancerous'].astype(str)\n",
    "data_extra['cellType'] = np.nan\n",
    "data_extra.loc[data_extra['isCancerous'] == '1', 'cellType'] = 2\n",
    "data_extra_unlabeled = data_extra[data_extra['cellType'].isna()]\n",
    "data_extra_labeled = data_extra[data_extra['cellType'] == 2]\n",
    "train_data = pd.concat([train_data, data_extra_labeled], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10906 validated image filenames belonging to 4 classes.\n",
      "Found 7394 validated image filenames belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qx/28c_j79n3zn_3rkwzsdcmn240000gn/T/ipykernel_2026/2991853267.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_extra_unlabeled['isCancerous'] = data_extra_unlabeled['isCancerous'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "train_data['cellType'] = train_data['cellType'].astype(str)\n",
    "data_extra_unlabeled['isCancerous'] = data_extra_unlabeled['isCancerous'].astype(str)\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_data,\n",
    "    directory='./Image_classification_data/patch_images',\n",
    "    x_col='ImageName',\n",
    "    y_col='cellType',\n",
    "    target_size=(27, 27),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "unlabeled_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "unlabeled_generator = unlabeled_datagen.flow_from_dataframe(\n",
    "    data_extra_unlabeled,\n",
    "    directory='./Image_classification_data/patch_images',\n",
    "    x_col='ImageName',\n",
    "    y_col='isCancerous',\n",
    "    target_size=(27, 27),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T19:28:35.889209Z",
     "end_time": "2023-04-27T19:28:36.045883Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 341it [00:31, 10.99it/s]                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - loss: 0.3758, supervised_loss: 0.3689, consistency_loss: 0.0069, accuracy: 0.8620\n",
      "Validation set： - loss: 0.5323, accuracy: 0.7993\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 341it [00:31, 10.97it/s]                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - loss: 0.3577, supervised_loss: 0.3513, consistency_loss: 0.0064, accuracy: 0.8697\n",
      "Validation set： - loss: 0.5333, accuracy: 0.7987\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 341it [00:31, 10.94it/s]                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - loss: 0.3552, supervised_loss: 0.3485, consistency_loss: 0.0067, accuracy: 0.8701\n",
      "Validation set： - loss: 0.5328, accuracy: 0.7990\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|████████████████████████▋                     | 124/231.0625 [00:11<00:09, 10.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [240]\u001B[0m, in \u001B[0;36m<cell line: 48>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     55\u001B[0m steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tqdm(total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(data_extra_unlabeled) \u001B[38;5;241m/\u001B[39m batch_size, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining\u001B[39m\u001B[38;5;124m\"\u001B[39m, ncols\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m progress_bar:\n\u001B[0;32m---> 58\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (x_batch, y_batch), (x_unlabeled, _) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(train_generator, unlabeled_generator):\n\u001B[1;32m     59\u001B[0m         progress_bar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     60\u001B[0m         loss, supervised_loss, consistency_loss, accuracy \u001B[38;5;241m=\u001B[39m apply_uda(x_batch, y_batch, x_unlabeled, model_categorical, optimizer)\n",
      "File \u001B[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/preprocessing/image.py:156\u001B[0m, in \u001B[0;36mIterator.__next__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/preprocessing/image.py:168\u001B[0m, in \u001B[0;36mIterator.next\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    165\u001B[0m     index_array \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_generator)\n\u001B[1;32m    166\u001B[0m \u001B[38;5;66;03m# The transformation of images is not under thread lock\u001B[39;00m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;66;03m# so it can be done in parallel\u001B[39;00m\n\u001B[0;32m--> 168\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_batches_of_transformed_samples\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex_array\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/preprocessing/image.py:377\u001B[0m, in \u001B[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001B[0;34m(self, index_array)\u001B[0m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(index_array):\n\u001B[1;32m    370\u001B[0m     img \u001B[38;5;241m=\u001B[39m image_utils\u001B[38;5;241m.\u001B[39mload_img(\n\u001B[1;32m    371\u001B[0m         filepaths[j],\n\u001B[1;32m    372\u001B[0m         color_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolor_mode,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    375\u001B[0m         keep_aspect_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeep_aspect_ratio,\n\u001B[1;32m    376\u001B[0m     )\n\u001B[0;32m--> 377\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mimage_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimg_to_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_format\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    378\u001B[0m     \u001B[38;5;66;03m# Pillow images should be closed after `load_img`,\u001B[39;00m\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;66;03m# but not PIL images.\u001B[39;00m\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(img, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclose\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/utils/image_utils.py:324\u001B[0m, in \u001B[0;36mimg_to_array\u001B[0;34m(img, data_format, dtype)\u001B[0m\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown data_format: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_format\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    321\u001B[0m \u001B[38;5;66;03m# Numpy array x has format (height, width, channel)\u001B[39;00m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;66;03m# or (channel, height, width)\u001B[39;00m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;66;03m# but original PIL image has format (width, height, channel)\u001B[39;00m\n\u001B[0;32m--> 324\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_format \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchannels_first\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/PIL/Image.py:675\u001B[0m, in \u001B[0;36mImage.__array__\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m    673\u001B[0m     new[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtobytes(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    674\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 675\u001B[0m     new[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtobytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ArrayData(new), dtype)\n",
      "File \u001B[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/PIL/Image.py:718\u001B[0m, in \u001B[0;36mImage.tobytes\u001B[0;34m(self, encoder_name, *args)\u001B[0m\n\u001B[1;32m    715\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m encoder_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m args \u001B[38;5;241m==\u001B[39m ():\n\u001B[1;32m    716\u001B[0m     args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode\n\u001B[0;32m--> 718\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    720\u001B[0m \u001B[38;5;66;03m# unpack data\u001B[39;00m\n\u001B[1;32m    721\u001B[0m e \u001B[38;5;241m=\u001B[39m _getencoder(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode, encoder_name, args)\n",
      "File \u001B[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/PIL/ImageFile.py:253\u001B[0m, in \u001B[0;36mImageFile.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    247\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[1;32m    248\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage file is truncated \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    249\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(b)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m bytes not processed)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    250\u001B[0m         )\n\u001B[1;32m    252\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[0;32m--> 253\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "consistency_weight = 1\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "\n",
    "# 定义数据增强策略\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "])\n",
    "\n",
    "# 训练循环\n",
    "def apply_uda(x_batch, y_batch, x_unlabeled, model, optimizer, training=True):\n",
    "    x_unlabeled_augmented = None\n",
    "    if x_unlabeled is not None:\n",
    "        # 对无标签数据进行数据增强\n",
    "        x_unlabeled_augmented = data_augmentation(x_unlabeled)\n",
    "\n",
    "    # 计算模型在原始无标签数据和增强无标签数据上的输出\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred_labeled = model(x_batch)\n",
    "        if x_unlabeled is not None:\n",
    "            y_pred_unlabeled = model(x_unlabeled)\n",
    "            y_pred_unlabeled_augmented = model(x_unlabeled_augmented)\n",
    "\n",
    "        # 计算有监督损失\n",
    "        supervised_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_batch, y_pred_labeled))\n",
    "\n",
    "        # 如果提供了无标签数据，则计算一致性损失\n",
    "        if x_unlabeled is not None:\n",
    "            consistency_loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(y_pred_unlabeled, y_pred_unlabeled_augmented))\n",
    "            total_loss = supervised_loss + consistency_weight * consistency_loss\n",
    "        else:\n",
    "            total_loss = supervised_loss\n",
    "\n",
    "        # 计算准确度\n",
    "        accuracy = tf.keras.metrics.categorical_accuracy(y_batch, y_pred_labeled)\n",
    "\n",
    "    if training:\n",
    "        # 反向传播和优化\n",
    "        grads = tape.gradient(total_loss, model_categorical.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model_categorical.trainable_variables))\n",
    "    return total_loss, supervised_loss, consistency_loss if x_unlabeled is not None else None, accuracy\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "min_val_loss = float('inf')\n",
    "num_epochs_no_improvement = 0\n",
    "# current_learning_rate = optimizer.learning_rate.numpy()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    total_loss = 0\n",
    "    total_supervised_loss = 0\n",
    "    total_consistency_loss = 0\n",
    "    total_accuracy = 0\n",
    "    steps = 0\n",
    "\n",
    "    with tqdm(total=len(data_extra_unlabeled) / batch_size, desc=\"Training\", ncols=100) as progress_bar:\n",
    "        for (x_batch, y_batch), (x_unlabeled, _) in zip(train_generator, unlabeled_generator):\n",
    "            progress_bar.update(1)\n",
    "            loss, supervised_loss, consistency_loss, accuracy = apply_uda(x_batch, y_batch, x_unlabeled, model_categorical, optimizer)\n",
    "            total_loss += loss\n",
    "            total_supervised_loss += supervised_loss\n",
    "            total_consistency_loss += consistency_loss\n",
    "            total_accuracy += tf.reduce_mean(accuracy)\n",
    "            steps += 1\n",
    "            # 检查是否已经处理了所有批次\n",
    "            if steps * batch_size >= len(train_data):\n",
    "                break\n",
    "\n",
    "    # 计算并打印平均损失和准确率\n",
    "    avg_loss = total_loss / steps\n",
    "    avg_supervised_loss = total_supervised_loss / steps\n",
    "    avg_consistency_loss = total_consistency_loss / steps\n",
    "    avg_accuracy = total_accuracy / steps\n",
    "    print(f\" - loss: {avg_loss.numpy():.4f}, supervised_loss: {avg_supervised_loss.numpy():.4f}, consistency_loss: {avg_consistency_loss.numpy():.4f}, accuracy: {avg_accuracy.numpy():.4f}\")\n",
    "\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    steps = 0\n",
    "    for x_batch, y_batch in validation_cellType_generator:\n",
    "        loss, _, _, accuracy = apply_uda(x_batch, y_batch, None, model_categorical, optimizer, False)\n",
    "        total_loss += loss\n",
    "        total_accuracy += tf.reduce_mean(accuracy)\n",
    "        steps += 1\n",
    "        # 检查是否需要更新学习率\n",
    "        # if loss < min_val_loss:\n",
    "        #     min_val_loss = loss\n",
    "        #     num_epochs_no_improvement = 0\n",
    "        # else:\n",
    "        #     num_epochs_no_improvement += 1\n",
    "        #\n",
    "        # if num_epochs_no_improvement >= 3:\n",
    "        #     current_learning_rate *= np.sqrt(0.1)\n",
    "        #     current_learning_rate = max(current_learning_rate, 0.5e-15)\n",
    "        #     optimizer.learning_rate.assign(current_learning_rate)\n",
    "        #     num_epochs_no_improvement = 0\n",
    "\n",
    "        if steps * batch_size >= len(test_data):\n",
    "            break\n",
    "\n",
    "    avg_loss = total_loss / steps\n",
    "    avg_accuracy = total_accuracy / steps\n",
    "    print(f\"Validation set： - loss: {avg_loss.numpy():.4f}, accuracy: {avg_accuracy.numpy():.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T19:09:47.080955Z",
     "end_time": "2023-04-27T19:10:13.458576Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
